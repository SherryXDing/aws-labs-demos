{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression with Amazon SageMaker XGBoost algorithm\n",
    "_**Distributed training for regression with Amazon SageMaker XGBoost script mode**_\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Setup](#Setup)\n",
    "  1. [Fetching the dataset](#Fetching-the-dataset)\n",
    "  2. [Data Ingestion](#Data-ingestion)\n",
    "3. [Training the XGBoost model](#Training-the-XGBoost-model)\n",
    "3. [Deploying the XGBoost model](#Deploying-the-XGBoost-model)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates the use of Amazon SageMaker XGBoost to train and host a regression model. [XGBoost (eXtreme Gradient Boosting)](https://xgboost.readthedocs.io) is a popular and efficient machine learning algorithm used for regression and classification tasks on tabular datasets. It implements a technique know as gradient boosting on trees, and performs remarkably well in machine learning competitions, and gets a lot of attention from customers. \n",
    "\n",
    "We use the [Abalone data](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression.html), originally from the [UCI data repository](https://archive.ics.uci.edu/ml/datasets/abalone). More details about the original dataset can be found [here](https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.names).  In this libsvm converted version, the nominal feature (Male/Female/Infant) has been converted into a real valued feature as required by XGBoost. Age of abalone is to be predicted from eight physical measurements.  \n",
    "\n",
    "---\n",
    "## Setup\n",
    "\n",
    "This notebook was tested in Amazon SageMaker Studio on a ml.t3.medium instance with Python 3 (Data Science) kernel.\n",
    "\n",
    "Let's start by specifying:\n",
    "1. The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "1. The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these. Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp with a the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-east-2\n",
      "CPU times: user 82.6 ms, sys: 0 ns, total: 82.6 ms\n",
      "Wall time: 156 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import sagemaker\n",
    "\n",
    "# Get a SageMaker-compatible role used by this Notebook Instance.\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "### update below values appropriately ###\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = 'sagemaker/xgboost-pipe-dist-script'\n",
    "#### \n",
    "\n",
    "print(region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching the dataset\n",
    "\n",
    "Following methods split the data into train/test/validation datasets and upload files to S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data ingestion\n",
    "\n",
    "Next, we read the dataset from the existing repository into memory, for preprocessing prior to training. This processing could be done *in situ* by Amazon Athena, Apache Spark in Amazon EMR, Amazon Redshift, etc., assuming the dataset is present in the appropriate location. Then, the next step would be to transfer the data to S3 for use in training. For small datasets, such as this one, reading into memory isn't onerous, though it would be for larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56.7 ms, sys: 784 µs, total: 57.5 ms\n",
      "Wall time: 284 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import pyarrow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "# Download the dataset and load into a pandas dataframe\n",
    "FILE_NAME = 'abalone.csv'\n",
    "s3.download_file(\"sagemaker-sample-files\", f\"datasets/tabular/uci_abalone/abalone.csv\", FILE_NAME)\n",
    "feature_names=['Sex', \n",
    "               'Length', \n",
    "               'Diameter', \n",
    "               'Height', \n",
    "               'Whole weight', \n",
    "               'Shucked weight', \n",
    "               'Viscera weight', \n",
    "               'Shell weight', \n",
    "               'Rings']\n",
    "data = pd.read_csv(FILE_NAME, \n",
    "                   header=None, \n",
    "                   names=feature_names)\n",
    "\n",
    "# SageMaker XGBoost has the convention of label in the first column\n",
    "data = data[feature_names[-1:] + feature_names[:-1]]\n",
    "data[\"Sex\"] = data[\"Sex\"].astype(\"category\").cat.codes\n",
    "\n",
    "# Split the downloaded data into train/test dataframes\n",
    "train, validation = np.split(data.sample(frac=1), [int(.8*len(data))])\n",
    "train_0, train_1 = np.split(train.sample(frac=1), [int(.5*len(train))])\n",
    "\n",
    "# requires PyArrow installed\n",
    "train_0.to_parquet('abalone_train_0.parquet')\n",
    "train_1.to_parquet('abalone_train_1.parquet')\n",
    "validation.to_parquet('abalone_validation.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 86.6 ms, sys: 151 µs, total: 86.7 ms\n",
      "Wall time: 301 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-2-240487350066/sagemaker/xgboost-pipe-dist-script/validation/abalone_validation.parquet'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sagemaker.Session().upload_data('abalone_train_0.parquet', \n",
    "                                bucket=bucket, \n",
    "                                key_prefix=prefix+'/'+'train')\n",
    "\n",
    "sagemaker.Session().upload_data('abalone_train_1.parquet', \n",
    "                                bucket=bucket, \n",
    "                                key_prefix=prefix+'/'+'train')\n",
    "\n",
    "sagemaker.Session().upload_data('abalone_validation.parquet', \n",
    "                                bucket=bucket, \n",
    "                                key_prefix=prefix+'/'+'validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a XGBoost script to train with \n",
    "\n",
    "SageMaker can now run an XGboost script using the XGBoost estimator. When executed on SageMaker a number of helpful environment variables are available to access properties of the training environment, such as:\n",
    "\n",
    "- `SM_MODEL_DIR`: A string representing the path to the directory to write model artifacts to. Any artifacts saved in this folder are uploaded to S3 for model hosting after the training job completes.\n",
    "- `SM_OUTPUT_DIR`: A string representing the filesystem path to write output artifacts to. Output artifacts may include checkpoints, graphs, and other files to save, not including model artifacts. These artifacts are compressed and uploaded to S3 to the same S3 prefix as the model artifacts.\n",
    "\n",
    "Supposing two input channels, 'train' and 'validation', were used in the call to the XGBoost estimator's fit() method, the following environment variables will be set, following the format `SM_CHANNEL_[channel_name]`:\n",
    "\n",
    "`SM_CHANNEL_TRAIN`: A string representing the path to the directory containing data in the 'train' channel\n",
    "`SM_CHANNEL_VALIDATION`: Same as above, but for the 'validation' channel.\n",
    "\n",
    "A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to model_dir so that it can be hosted later. Hyperparameters are passed to your script as arguments and can be retrieved with an argparse.ArgumentParser instance. For example, the script that we will run in this notebook is provided as the accompanying file (`abalone_pipe.py`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the container imports your training script, always put your training code in a main guard `(if __name__=='__main__':)` so that the container does not inadvertently run your training code at the wrong point in execution.\n",
    "\n",
    "For more information about training environment variables, please visit https://github.com/aws/sagemaker-containers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the XGBoost model\n",
    "\n",
    "After setting training parameters, we kick off training, and poll for status until training is completed, which in this example, takes between few minutes.\n",
    "\n",
    "To run our training script on SageMaker, we construct a sagemaker.xgboost.estimator.XGBoost estimator, which accepts several constructor arguments:\n",
    "\n",
    "* __entry_point__: The path to the Python script SageMaker runs for training and prediction.\n",
    "* __role__: Role ARN\n",
    "* __train_instance_type__ *(optional)*: The type of SageMaker instances for training. __Note__: Because Scikit-learn does not natively support GPU training, Sagemaker Scikit-learn does not currently support training on GPU instance types.\n",
    "* __sagemaker_session__ *(optional)*: The session used to train on Sagemaker.\n",
    "* __hyperparameters__ *(optional)*: A dictionary passed to the train function as hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "        \"max_depth\": \"5\",\n",
    "        \"eta\": \"0.2\",\n",
    "        \"gamma\": \"4\",\n",
    "        \"min_child_weight\": \"6\",\n",
    "        \"subsample\": \"0.7\",\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"num_round\": \"50\",\n",
    "        \"verbosity\": \"2\",\n",
    "        \"is_pipe_mode\": True\n",
    "}\n",
    "\n",
    "instance_type = \"ml.m5.2xlarge\"\n",
    "output_path = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "content_type = \"parquet\"\n",
    "input_mode = \"Pipe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Source distributed script mode\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "session = Session(boto_session=boto_session)\n",
    "script_path = 'abalone_pipe.py'\n",
    "\n",
    "xgb_script_mode_estimator = XGBoost(\n",
    "    entry_point=script_path,\n",
    "    framework_version='1.2-1', # Note: framework_version is mandatory\n",
    "    hyperparameters=hyperparams,\n",
    "    role=role,\n",
    "    instance_count=2,\n",
    "    instance_type=instance_type,\n",
    "    output_path=output_path,\n",
    "    input_mode=input_mode)\n",
    "\n",
    "train_input = TrainingInput(\"s3://{}/{}/{}/\".format(bucket, prefix, \"train\"), distribution='ShardedByS3Key', content_type=content_type)\n",
    "validation_input = TrainingInput(\"s3://{}/{}/{}/\".format(bucket, prefix, \"validation\"), distribution='FullyReplicated', content_type=content_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train XGBoost Estimator on abalone data \n",
    "\n",
    "\n",
    "Training is as simple as calling `fit` on the Estimator. This will start a SageMaker Training job that will download the data, invoke the entry point code (in the provided script file), and save any model artifacts that the script creates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-23 17:33:12 Starting - Starting the training job...\n",
      "2021-04-23 17:33:34 Starting - Launching requested ML instancesProfilerReport-1619199191: InProgress\n",
      "......\n",
      "2021-04-23 17:34:35 Starting - Preparing the instances for training......\n",
      "2021-04-23 17:35:41 Downloading - Downloading input data\n",
      "2021-04-23 17:35:41 Training - Downloading the training image...\n",
      "2021-04-23 17:35:55 Training - Training image download completed. Training in progress.\u001b[35m[2021-04-23 17:35:56.309 ip-10-0-198-51.us-east-2.compute.internal:1 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[35mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35mINFO:sagemaker_xgboost_container.training:Invoking user training script.\u001b[0m\n",
      "\u001b[35mINFO:sagemaker-containers:Module abalone_pipe does not provide a setup.py. \u001b[0m\n",
      "\u001b[35mGenerating setup.py\u001b[0m\n",
      "\u001b[35mINFO:sagemaker-containers:Generating setup.cfg\u001b[0m\n",
      "\u001b[35mINFO:sagemaker-containers:Generating MANIFEST.in\u001b[0m\n",
      "\u001b[35mINFO:sagemaker-containers:Installing module with the following command:\u001b[0m\n",
      "\u001b[35m/miniconda3/bin/python3 -m pip install . \u001b[0m\n",
      "\u001b[35mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: abalone-pipe\n",
      "  Building wheel for abalone-pipe (setup.py): started\n",
      "  Building wheel for abalone-pipe (setup.py): finished with status 'done'\n",
      "  Created wheel for abalone-pipe: filename=abalone_pipe-1.0.0-py2.py3-none-any.whl size=6673 sha256=dba94a80be0f3b5ff2fa2abe13034af659076f5167de36c6b564506101964e9e\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-lf6fhzv6/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\u001b[0m\n",
      "\u001b[35mSuccessfully built abalone-pipe\u001b[0m\n",
      "\u001b[35mInstalling collected packages: abalone-pipe\u001b[0m\n",
      "\u001b[35mSuccessfully installed abalone-pipe-1.0.0\u001b[0m\n",
      "\u001b[35mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35mINFO:sagemaker-containers:Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[35mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"validation\": \"/opt/ml/input/data/validation\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"framework_module\": \"sagemaker_xgboost_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"max_depth\": \"5\",\n",
      "        \"objective\": \"reg:squarederror\",\n",
      "        \"eta\": \"0.2\",\n",
      "        \"is_pipe_mode\": true,\n",
      "        \"num_round\": \"50\",\n",
      "        \"subsample\": \"0.7\",\n",
      "        \"gamma\": \"4\",\n",
      "        \"min_child_weight\": \"6\",\n",
      "        \"verbosity\": \"2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"validation\": {\n",
      "            \"ContentType\": \"parquet\",\n",
      "            \"TrainingInputMode\": \"Pipe\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"ContentType\": \"parquet\",\n",
      "            \"TrainingInputMode\": \"Pipe\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": false,\n",
      "    \"job_name\": \"sagemaker-xgboost-2021-04-23-17-33-11-804\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-240487350066/sagemaker-xgboost-2021-04-23-17-33-11-804/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"abalone_pipe\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"abalone_pipe.py\"\u001b[0m\n",
      "\u001b[35m}\n",
      "\u001b[0m\n",
      "\u001b[35mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"eta\":\"0.2\",\"gamma\":\"4\",\"is_pipe_mode\":true,\"max_depth\":\"5\",\"min_child_weight\":\"6\",\"num_round\":\"50\",\"objective\":\"reg:squarederror\",\"subsample\":\"0.7\",\"verbosity\":\"2\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=abalone_pipe.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"parquet\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"Pipe\"},\"validation\":{\"ContentType\":\"parquet\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"Pipe\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=abalone_pipe\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_xgboost_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-east-2-240487350066/sagemaker-xgboost-2021-04-23-17-33-11-804/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-2\",\"framework_module\":\"sagemaker_xgboost_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"eta\":\"0.2\",\"gamma\":\"4\",\"is_pipe_mode\":true,\"max_depth\":\"5\",\"min_child_weight\":\"6\",\"num_round\":\"50\",\"objective\":\"reg:squarederror\",\"subsample\":\"0.7\",\"verbosity\":\"2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"parquet\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"Pipe\"},\"validation\":{\"ContentType\":\"parquet\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"Pipe\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":false,\"job_name\":\"sagemaker-xgboost-2021-04-23-17-33-11-804\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-240487350066/sagemaker-xgboost-2021-04-23-17-33-11-804/source/sourcedir.tar.gz\",\"module_name\":\"abalone_pipe\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"abalone_pipe.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--eta\",\"0.2\",\"--gamma\",\"4\",\"--is_pipe_mode\",\"True\",\"--max_depth\",\"5\",\"--min_child_weight\",\"6\",\"--num_round\",\"50\",\"--objective\",\"reg:squarederror\",\"--subsample\",\"0.7\",\"--verbosity\",\"2\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[35mSM_HP_MAX_DEPTH=5\u001b[0m\n",
      "\u001b[35mSM_HP_OBJECTIVE=reg:squarederror\u001b[0m\n",
      "\u001b[35mSM_HP_ETA=0.2\u001b[0m\n",
      "\u001b[35mSM_HP_IS_PIPE_MODE=true\u001b[0m\n",
      "\u001b[35mSM_HP_NUM_ROUND=50\u001b[0m\n",
      "\u001b[35mSM_HP_SUBSAMPLE=0.7\u001b[0m\n",
      "\u001b[35mSM_HP_GAMMA=4\u001b[0m\n",
      "\u001b[35mSM_HP_MIN_CHILD_WEIGHT=6\u001b[0m\n",
      "\u001b[35mSM_HP_VERBOSITY=2\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/miniconda3/bin:/:/miniconda3/lib/python/site-packages/xgboost/dmlc-core/tracker:/miniconda3/lib/python37.zip:/miniconda3/lib/python3.7:/miniconda3/lib/python3.7/lib-dynload:/miniconda3/lib/python3.7/site-packages\n",
      "\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[35m/miniconda3/bin/python3 -m abalone_pipe --eta 0.2 --gamma 4 --is_pipe_mode True --max_depth 5 --min_child_weight 6 --num_round 50 --objective reg:squarederror --subsample 0.7 --verbosity 2\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m[2021-04-23 17:35:56.640 ip-10-0-220-122.us-east-2.compute.internal:1 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Invoking user training script.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Module abalone_pipe does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Generating setup.cfg\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python3 -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: abalone-pipe\n",
      "  Building wheel for abalone-pipe (setup.py): started\n",
      "  Building wheel for abalone-pipe (setup.py): finished with status 'done'\n",
      "  Created wheel for abalone-pipe: filename=abalone_pipe-1.0.0-py2.py3-none-any.whl size=6673 sha256=ec8a9c28059035e8305a229211f2310ef971d8e565d610b5e32ad677f91de0d9\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-x5ueluot/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\u001b[0m\n",
      "\u001b[34mSuccessfully built abalone-pipe\u001b[0m\n",
      "\u001b[34mInstalling collected packages: abalone-pipe\u001b[0m\n",
      "\u001b[34mSuccessfully installed abalone-pipe-1.0.0\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"validation\": \"/opt/ml/input/data/validation\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_xgboost_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"max_depth\": \"5\",\n",
      "        \"objective\": \"reg:squarederror\",\n",
      "        \"eta\": \"0.2\",\n",
      "        \"is_pipe_mode\": true,\n",
      "        \"num_round\": \"50\",\n",
      "        \"subsample\": \"0.7\",\n",
      "        \"gamma\": \"4\",\n",
      "        \"min_child_weight\": \"6\",\n",
      "        \"verbosity\": \"2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"validation\": {\n",
      "            \"ContentType\": \"parquet\",\n",
      "            \"TrainingInputMode\": \"Pipe\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"ContentType\": \"parquet\",\n",
      "            \"TrainingInputMode\": \"Pipe\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-xgboost-2021-04-23-17-33-11-804\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-240487350066/sagemaker-xgboost-2021-04-23-17-33-11-804/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"abalone_pipe\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"abalone_pipe.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"eta\":\"0.2\",\"gamma\":\"4\",\"is_pipe_mode\":true,\"max_depth\":\"5\",\"min_child_weight\":\"6\",\"num_round\":\"50\",\"objective\":\"reg:squarederror\",\"subsample\":\"0.7\",\"verbosity\":\"2\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=abalone_pipe.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"parquet\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"Pipe\"},\"validation\":{\"ContentType\":\"parquet\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"Pipe\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=abalone_pipe\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_xgboost_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-240487350066/sagemaker-xgboost-2021-04-23-17-33-11-804/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_xgboost_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"eta\":\"0.2\",\"gamma\":\"4\",\"is_pipe_mode\":true,\"max_depth\":\"5\",\"min_child_weight\":\"6\",\"num_round\":\"50\",\"objective\":\"reg:squarederror\",\"subsample\":\"0.7\",\"verbosity\":\"2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"parquet\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"Pipe\"},\"validation\":{\"ContentType\":\"parquet\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"Pipe\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-xgboost-2021-04-23-17-33-11-804\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-240487350066/sagemaker-xgboost-2021-04-23-17-33-11-804/source/sourcedir.tar.gz\",\"module_name\":\"abalone_pipe\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"abalone_pipe.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--eta\",\"0.2\",\"--gamma\",\"4\",\"--is_pipe_mode\",\"True\",\"--max_depth\",\"5\",\"--min_child_weight\",\"6\",\"--num_round\",\"50\",\"--objective\",\"reg:squarederror\",\"--subsample\",\"0.7\",\"--verbosity\",\"2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_DEPTH=5\u001b[0m\n",
      "\u001b[34mSM_HP_OBJECTIVE=reg:squarederror\u001b[0m\n",
      "\u001b[34mSM_HP_ETA=0.2\u001b[0m\n",
      "\u001b[34mSM_HP_IS_PIPE_MODE=true\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_ROUND=50\u001b[0m\n",
      "\u001b[34mSM_HP_SUBSAMPLE=0.7\u001b[0m\n",
      "\u001b[34mSM_HP_GAMMA=4\u001b[0m\n",
      "\u001b[34mSM_HP_MIN_CHILD_WEIGHT=6\u001b[0m\n",
      "\u001b[34mSM_HP_VERBOSITY=2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/miniconda3/bin:/:/miniconda3/lib/python/site-packages/xgboost/dmlc-core/tracker:/miniconda3/lib/python37.zip:/miniconda3/lib/python3.7:/miniconda3/lib/python3.7/lib-dynload:/miniconda3/lib/python3.7/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python3 -m abalone_pipe --eta 0.2 --gamma 4 --is_pipe_mode True --max_depth 5 --min_child_weight 6 --num_round 50 --objective reg:squarederror --subsample 0.7 --verbosity 2\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mtask NULL connected to the tracker\u001b[0m\n",
      "\u001b[35mtask NULL connected to the tracker\u001b[0m\n",
      "\u001b[35mtask NULL got new rank 0\u001b[0m\n",
      "\u001b[34mtask NULL got new rank 1\u001b[0m\n",
      "\u001b[34mtask NULL connected to the tracker\u001b[0m\n",
      "\u001b[35mtask NULL connected to the tracker\u001b[0m\n",
      "\u001b[35mtask NULL got new rank 0\u001b[0m\n",
      "\u001b[35m[17:36:07] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:07] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 42 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:08] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:08] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 44 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:08] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:08] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 50 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:08] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34mtask NULL got new rank 1\u001b[0m\n",
      "\u001b[34m[17:36:07] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:07] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 42 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:08] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:08] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 44 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:08] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:08] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 50 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:08] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:08] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 56 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:08] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:08] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 56 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:08] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:08] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 58 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:08] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:08] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 56 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:08] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:08] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:08] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:08] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 58 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:08] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:09] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 60 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:09] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:09] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 30 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:09] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:09] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:08] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 58 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:08] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:08] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 56 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:08] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:08] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:08] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:08] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 58 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:09] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:09] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 60 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:09] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:09] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 30 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:09] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:09] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:09] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:09] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:09] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 56 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:09] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:09] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 44 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:09] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:09] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:09] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:09] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 56 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:09] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:10] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 42 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:10] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:10] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 40 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:10] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:10] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 46 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:10] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:09] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 56 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:09] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:09] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 44 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:09] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:09] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:09] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:09] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 56 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:09] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:10] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 42 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:10] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:10] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 40 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:10] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:10] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 46 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:10] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:10] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 44 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:10] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:10] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 50 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:10] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:10] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 44 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:10] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:10] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 36 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:10] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:11] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 22 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:11] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:10] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 44 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:10] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:10] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 50 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:10] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:10] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 44 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:10] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:10] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 36 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:10] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:11] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 22 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:11] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:11] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 46 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:11] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:11] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 38 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:11] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:11] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 32 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:11] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:11] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 34 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:12] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:11] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 46 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:11] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:11] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 38 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:11] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:11] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 32 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:11] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:11] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 34 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:12] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:12] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 40 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:12] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 40 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:12] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:12] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 46 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:12] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:12] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 58 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:12] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:12] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 38 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:12] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:13] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 42 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:13] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:13] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 34 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:13] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:13] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 44 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:13] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:12] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:12] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 46 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:12] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:12] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 58 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:12] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:12] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 38 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:12] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:13] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 42 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:13] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:13] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 34 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:13] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:13] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 44 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:13] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:13] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 32 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:13] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:13] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 36 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:13] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:13] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 46 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:13] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:13] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:13] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:14] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 50 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:14] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:14] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 40 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:14] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:14] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 44 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:14] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:13] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 32 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:13] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:13] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 36 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:13] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:13] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 46 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:13] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:13] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 52 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:13] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:14] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 50 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:14] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:14] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 40 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:14] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:14] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 44 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:14] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:14] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 38 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:14] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:14] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 44 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:14] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:14] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 34 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:14] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:14] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 38 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:14] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 58 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:15] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 30 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:15] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 36 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:15] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:14] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 38 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:14] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:14] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 44 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:14] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:14] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 34 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:14] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:14] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 38 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:14] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 58 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:15] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 30 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:15] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 36 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:15] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[34m[17:36:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 32 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:15] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 32 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[35m[17:36:15] WARNING: ../src/gbm/gbtree.cc:129: Tree method is automatically selected to be 'approx' for distributed training.\u001b[0m\n",
      "\u001b[35m[17:36:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 42 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17:36:15] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 42 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\n",
      "2021-04-23 17:36:35 Uploading - Uploading generated training model\n",
      "2021-04-23 17:36:35 Completed - Training job completed\n",
      "Training seconds: 138\n",
      "Billable seconds: 138\n"
     ]
    }
   ],
   "source": [
    "xgb_script_mode_estimator.fit({'train': train_input, 'validation': validation_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the XGBoost model\n",
    "\n",
    "After training, we can use the estimator to create an Amazon SageMaker endpoint – a hosted and managed prediction service that we can use to perform inference.\n",
    "\n",
    "You can also optionally specify other functions to customize the behavior of deserialization of the input request (`input_fn()`), serialization of the predictions (`output_fn()`), and how predictions are made (`predict_fn()`). The defaults work for our current use-case so we don’t need to define them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "predictor = xgb_script_mode_estimator.deploy(initial_instance_count=1, \n",
    "                                             instance_type=\"ml.m5.2xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Delete the Endpoint\n",
    "\n",
    "If you're done with this exercise, please run the delete_endpoint line in the cell below.  This will remove the hosted endpoint and avoid any charges from a stray instance being left on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

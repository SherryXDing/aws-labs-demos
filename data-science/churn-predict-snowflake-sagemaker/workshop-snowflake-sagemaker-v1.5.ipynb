{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Churn Predictive Analytics using Amazon SageMaker and Snowflake\n",
    "\n",
    "---\n",
    "## Background\n",
    "\n",
    "The purpose of this lab is to demonstrate the basics of building an advanced analytics solution using Amazon SageMaker on data stored in Snowflake. In this notebook we will create a customer churn analytics solution by training an [Autogluon](https://github.com/awslabs/autogluon) churn model, and batching churn prediction scores back into Snowflake to power analytics. \n",
    "\n",
    "#### Learning Objectives \n",
    "\n",
    " - Learn how to query ground truth data from Snowflake into a pandas dataframe for exploration and feature engineering.\n",
    " - Train an [AutoGluon](https://github.com/awslabs/autogluon) model to perform churn prediction.\n",
    " - Learn how to run a Batch Transform job to batch predictions.\n",
    " - Upload the Churn Score results back to Snowflake to perform basic analysis. \n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "\n",
    "In summary:\n",
    " - You've built the lab environment using this CloudFormation [template](https://snowflake-corp-se-workshop.s3-us-west-1.amazonaws.com/sagemaker-snowflake-devdays-v1.5/sagemaker/snowflake-sagemaker-notebook-v1.5.yaml). This template installs the Snowflake python connector within your Jupyter instance.\n",
    " - You've taken note of the Snowflake credentials in the lab guide.\n",
    " - This notebook should be running in your default VPC. \n",
    " - Snowflake traffic uses port 443.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import Python libraries required by this notebook.\n",
    "\n",
    "We also obtain a reference to the IAM role attached to this notebook. This IAM role was created when you ran the CloudFormation Template. It provides you the permission to run the various SageMaker commands in this lab, and access data in your S3 bucket.\n",
    "\n",
    "In practice, ensure your roles only provide minimum privileges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from IPython.display import display\n",
    "from time import strftime, gmtime\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import AlgorithmEstimator, get_execution_role\n",
    "from sagemaker.predictor import RealTimePredictor, csv_serializer, StringDeserializer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "print(\"IAM role ARN: {}\".format(role))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now let's set the S3 bucket and prefix that we'll be using for staging our data for training and batch inference. This bucket should be created within the same region as where you deployed your resources using the CloudFormation template. \n",
    "\n",
    "- Replace <<'REPLACE WITH YOUR BUCKET NAME'>> with the name of your bucket.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: bucket = 'snowflake-sagemaker-workshop-your-accountid'\n",
    "bucket = '<<REPLACE WITH YOUR BUCKET NAME>>'\n",
    "prefix = 'churn-analytics-lab'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Access our Data\n",
    "\n",
    "Mobile operators have historical records on which customers ultimately ended up churning and which continued using the service. We can use this historical information to construct an ML model of one mobile operator’s churn using a process called training. After training the model, we can pass the profile information of an arbitrary customer (the same profile information that we used to train the model) to the model, and have the model predict whether this customer is going to churn. Of course, we expect the model to make mistakes–after all, predicting the future is tricky business! But I’ll also show how to deal with prediction errors.\n",
    "\n",
    "The dataset we use is publicly available and was mentioned in the book [Discovering Knowledge in Data](https://www.amazon.com/dp/0470908742/) by Daniel T. Larose. It is attributed by the author to the University of California Irvine Repository of Machine Learning Datasets.  In the previous steps, this dataset was loaded into the CUSTOMER_CHURN table in your Snowflake instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide the connection and credentials required to connect to your Snowflake account. You'll need to modify the cell below with the appropriate **ACCOUNT** for your Snowflake trial. If you followed the lab guide instructions, the username and password below will work. \n",
    "\n",
    ">**NOTE:** For Snowflake accounts in regions **other than US WEST** add the Region ID after a period <ACCOUNT>.<REGION ID> i.e. XYZ123456.US-EAST-1.\n",
    "\n",
    "In practice, security standards might prohibit you from providing credentials in clear text. As a best practice in production, you should utilize a service like [AWS Secrets Manager](https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html) to manage your database credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "# Connecting to Snowflake using the default authenticator\n",
    "ctx = snowflake.connector.connect(\n",
    "  user='sagemaker',\n",
    "  password='AWSSF123',\n",
    "  account='<<ACCOUNT>>',\n",
    "  warehouse='SAGEMAKER_WH',\n",
    "  database='ML_WORKSHOP',\n",
    "  schema='PUBLIC'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Exploration\n",
    "\n",
    "Now we can query Snowflake from our SageMaker Notebook. \n",
    "\n",
    "In practice, the data table will often contain more data than what is practical to operate on within a notebook instance, or relevant attributes are spread across multiple tables. Being able to run SQL queries and loading the data into a pandas dataframe will be helpful during the initial stages of development. Check out the Spark integration for a fully scalable solution. [Snowflake Connector for Spark](https://docs.snowflake.net/manuals/user-guide/spark-connector.html)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Snowflake Data\n",
    "cs=ctx.cursor()\n",
    "allrows=cs.execute(\"\"\"select Cust_ID,STATE,ACCOUNT_LENGTH,AREA_CODE,PHONE,INTL_PLAN,VMAIL_PLAN,VMAIL_MESSAGE,\n",
    "                   DAY_MINS,DAY_CALLS,DAY_CHARGE,EVE_MINS,EVE_CALLS,EVE_CHARGE,NIGHT_MINS,NIGHT_CALLS,\n",
    "                   NIGHT_CHARGE,INTL_MINS,INTL_CALLS,INTL_CHARGE,CUSTSERV_CALLS,\n",
    "                   CHURN from CUSTOMER_CHURN \"\"\").fetchall()\n",
    "\n",
    "churn = pd.DataFrame(allrows)\n",
    "churn.columns=['Cust_id','State','Account Length','Area Code','Phone','Intl Plan', 'VMail Plan', 'VMail Message','Day Mins',\n",
    "            'Day Calls', 'Day Charge', 'Eve Mins', 'Eve Calls', 'Eve Charge', 'Night Mins', 'Night Calls','Night Charge',\n",
    "            'Intl Mins','Intl Calls','Intl Charge','CustServ Calls', 'Churn']\n",
    "\n",
    "pd.set_option('display.max_columns', 500)     # Make sure we can see all of the columns\n",
    "pd.set_option('display.max_rows', 10)         # Keep the output on one page\n",
    "churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "By modern standards, it’s a relatively small dataset, with only 3,333 records, where each record uses 21 attributes to describe the profile of a customer of an unknown US mobile operator. The attributes are:\n",
    "\n",
    "- `State`: the US state in which the customer resides, indicated by a two-letter abbreviation; for example, OH or NJ\n",
    "- `Account Length`: the number of days that this account has been active\n",
    "- `Area Code`: the three-digit area code of the corresponding customer’s phone number\n",
    "- `Phone`: the remaining seven-digit phone number\n",
    "- `Int’l Plan`: whether the customer has an international calling plan: yes/no\n",
    "- `VMail Plan`: whether the customer has a voice mail feature: yes/no\n",
    "- `VMail Message`: presumably the average number of voice mail messages per month\n",
    "- `Day Mins`: the total number of calling minutes used during the day\n",
    "- `Day Calls`: the total number of calls placed during the day\n",
    "- `Day Charge`: the billed cost of daytime calls\n",
    "- `Eve Mins, Eve Calls, Eve Charge`: the billed cost for calls placed during the evening\n",
    "- `Night Mins`, `Night Calls`, `Night Charge`: the billed cost for calls placed during nighttime\n",
    "- `Intl Mins`, `Intl Calls`, `Intl Charge`: the billed cost for international calls\n",
    "- `CustServ Calls`: the number of calls placed to Customer Service\n",
    "- `Churn`: whether the customer left the service: true/false\n",
    "\n",
    "The last attribute, `Churn`, is known as the target attribute–the attribute that we want the ML model to predict.  Because the target attribute is binary, our model will be performing binary prediction, also known as binary classification.\n",
    "\n",
    "Let's begin exploring the data:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency tables for each categorical feature\n",
    "for column in churn.select_dtypes(include=['object']).columns:\n",
    "    display(pd.crosstab(index=churn[column], columns='% observations', normalize='columns'))\n",
    "\n",
    "# Histograms for each numeric features\n",
    "display(churn.describe())\n",
    "%matplotlib inline\n",
    "hist = churn.hist(bins=30, sharey=True, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We can see immediately that:\n",
    "- `State` appears to be quite evenly distributed\n",
    "- `Phone` takes on too many unique values to be of any practical use.  It's possible parsing out the prefix could have some value, but without more context on how these are allocated, we should avoid using it.\n",
    "- Only 14% of customers churned, so there is some class imabalance, but nothing extreme.\n",
    "- Most of the numeric features are surprisingly nicely distributed, with many showing bell-like gaussianity.  `VMail Message` being a notable exception (and `Area Code` showing up as a feature we should convert to non-numeric).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn = churn.drop('Phone', axis=1)\n",
    "churn['Area Code'] = churn['Area Code'].astype(object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next, let's look at the relationship between each of the features and our target variable.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in churn.select_dtypes(include=['object']).columns:\n",
    "    if column != 'Churn':\n",
    "        display(pd.crosstab(index=churn[column], columns=churn['Churn'], normalize='columns'))\n",
    "\n",
    "for column in churn.select_dtypes(exclude=['object']).columns:\n",
    "    print(column)\n",
    "    hist = churn[[column, 'Churn']].hist(by='Churn', bins=30)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Interestingly we see that churners appear:\n",
    "- Fairly evenly distributed geographically\n",
    "- More likely to have an international plan\n",
    "- Less likely to have a voicemail plan\n",
    "- To exhibit some bimodality in daily minutes (either higher or lower than the average for non-churners)\n",
    "- To have a larger number of customer service calls (which makes sense as we'd expect customers who experience lots of problems may be more likely to churn)\n",
    "\n",
    "In addition, we see that churners take on very similar distributions for features like `Day Mins` and `Day Charge`.  That's not surprising as we'd expect minutes spent talking to correlate with charges.  Let's dig deeper into the relationships between our features.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(churn.corr())\n",
    "pd.plotting.scatter_matrix(churn, figsize=(18, 18))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We see several features that essentially have 100% correlation with one another.  Including these feature pairs in some machine learning algorithms can create catastrophic problems, while in others it will only introduce minor redundancy and bias.  Let's remove one feature from each of the highly correlated pairs: Day Charge from the pair with Day Mins, Night Charge from the pair with Night Mins, Intl Charge from the pair with Intl Mins:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn = churn.drop(['Day Charge', 'Eve Charge', 'Night Charge', 'Intl Charge'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Automate Feature Engineering using AutoML\n",
    "\n",
    "\n",
    "Our data is now prep for ML. Normally, we would experiment with potential algorithms and performing additional feature engineering tasks to craft features with potentially stronger signals.\n",
    "\n",
    "We're going to shortcut this process of using one of our Tabular AutoML options: AutoGluon. AutoGluon is an toolkit created by AWS Labs. Information about the contributors and the research work is documented in this [paper](https://arxiv.org/pdf/2003.06505.pdf).\n",
    "\n",
    "Amazon SageMaker provides easy ways to train and deploy MXNet/Gluon based models simply by bringing your own script. AutoGluon is also conveniently packaged as a Marketplace [product](https://aws.amazon.com/marketplace/pp/prodview-n4zf5pmjt7ism) for SageMaker, so we don't need to worry about the operational heavy-lifting involved in maintaining containers and the runtime environments. We're going to use this product today in our workshop.\n",
    "\n",
    "SageMaker provides other Tabular AutoML options via [Autopilot](https://aws.amazon.com/sagemaker/autopilot/). There are trade-offs between options. For this small dataset, AutoGluon runs much faster and produces great results. Autopilot runs distributed and the overhead is prohibitive for the purpose of this workshop. However, the serverless  and native distributed setup is more practical for larger datasets, and is easier to manage. \n",
    "\n",
    "Tabular AutoML automates the experimentation process of automating data analysis, feature engineering, exploring candidate algorithms and pipelines, hyperparameter tuning and ensemble generation. This potentially enables non-ML-experts to produce high-quality ML models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalize our Datasets\n",
    "\n",
    "And now let's split the data into training and test sets.  This will help prevent overfitting, and allow us to test the models accuracy on unseen data. \n",
    "\n",
    "Traditionally, you would create a third validation set, which would be used during training. The validation set would be used in each training epoch to evaluate progress and help monitor overfitting issues. AutoGluon creates the validation set for us automatically.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_split_data = churn.drop(['Cust_id'], axis=1)\n",
    "train_data, test_data = np.split(to_split_data.sample(frac=1, random_state=1729), [int(0.9 * len(to_split_data))])\n",
    "train_data.to_csv('train.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's preview our training set. Looks like a normal database table! AutoGluon will automatically profile your dataset and identify the prescence of catagorical attributes like *State* and apply appropriate transformations like *one-hot-encoding*.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "display(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "SageMaker training ingests training data from S3, so we'll need to stage our datasets in our S3 bucket.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Subscribe to AutoGluon in the AWS Marketplace\n",
    "\n",
    "1. **Follow [this URL](https://aws.amazon.com/marketplace/pp/Amazon-Web-Services-AutoGluon-Tabular/prodview-n4zf5pmjt7ism) to the AutoGluon-Tabular product page.**\n",
    "2. Select the orange \"Continue to Subscribe\" button.\n",
    "3. Run the helper function below to identify the AWS resource ID (ARN) of your AutoGluon Marketplace algorithm.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOGLUON_PRODUCT = \"autogluon-tabular-v2-a11d018f6028f8192e60553704ee3d97\"\n",
    "def get_algorithm_arn(region, algo_name):\n",
    "    acct_mapping = {\n",
    "        \"ap-northeast-1\" : \"977537786026\",\n",
    "        \"ap-northeast-2\" : \"745090734665\",\n",
    "        \"ap-southeast-1\" : \"192199979996\",\n",
    "        \"ap-southeast-2\" : \"666831318237\",\n",
    "        \"us-east-1\"      : \"865070037744\",\n",
    "        \"eu-central-1\"   : \"446921602837\",\n",
    "        \"ap-south-1\"     : \"077584701553\",\n",
    "        \"sa-east-1\"      : \"270155090741\",\n",
    "        \"ca-central-1\"   : \"470592106596\",\n",
    "        \"eu-west-1\"      : \"985815980388\",\n",
    "        \"eu-west-2\"      : \"856760150666\",\n",
    "        \"eu-west-3\"      : \"843114510376\",\n",
    "        \"eu-north-1\"     : \"136758871317\",\n",
    "        \"us-west-1\"      : \"382657785993\",\n",
    "        \"us-east-2\"      : \"057799348421\",\n",
    "        \"us-west-2\"      : \"594846645681\"\n",
    "    }\n",
    "        \n",
    "    return \"arn:aws:sagemaker:{}:{}:algorithm/{}\".format(region, acct_mapping[region], algo_name)\n",
    "    \n",
    "algorithm_arn = get_algorithm_arn(boto3.Session().region_name, AUTOGLUON_PRODUCT)\n",
    "print(\"The Tabular AutoGluon ARN in your region is {}.\".format(algorithm_arn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Training\n",
    "\n",
    "Subscribing to the AutoGluon Marketplace algorithm provides you access to a SageMaker compatible container for the AutoGluon algorithm. This Marketplace algorithm is managed by AWS and doesn't have additonal software costs.\n",
    "\n",
    "Once subscribed, Marketplace algorithms are similar to SageMaker built-in algorithms. These algorithms can be trained and deployed with \"low-to-no-code\". \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll configure our algorithm for remote training (Note: you can configure and launch the job using the AWS console as an alternative to the SDK).\n",
    "\n",
    "1. **Hyperparamters**: AutoML algorithms like AutoGluon are designed to automate hyperparameter tuning using hyperparamter search algorithms like Bayesian Optimization. Thus, setting hyperparameters are optional. However, you can override the defaults. We'll use the default configurations in this lab, so we only need to identify the name of the target label column. The other configurations are commented out and serve as examples.\n",
    "\n",
    "2. **Infrastructure**: We're using SageMaker's remote training service, so we need to specify the infrastructure to allocate. Since we're using a Marketplace product, we need to be aware of the subset of supported instances. \n",
    "\n",
    "3. **Data**: lastly, we need to identify the location of our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    #\"hyperparameters\": {\n",
    "    #    \"NN\":{\"num_epochs\": \"1\"}\n",
    "    #},\n",
    "    #\"auto_stack\": \"True\",\n",
    "    \"label\": \"Churn\"\n",
    "}\n",
    "\n",
    "compatible_training_instance_type='ml.m5.4xlarge' \n",
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "Now, we create an Estimator object which describes our training job: security settings, infrastructure, data, and algorithms.\n",
    "\n",
    "Executing the fit() method results in an API call to the SageMaker service to build your training cluster and execute the training job.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autogluon = AlgorithmEstimator(algorithm_arn=algorithm_arn, \n",
    "                                  role=role, \n",
    "                                  train_instance_count=1, \n",
    "                                  train_instance_type=compatible_training_instance_type, \n",
    "                                  sagemaker_session=sess, \n",
    "                                  base_job_name='autogluon',\n",
    "                                  hyperparameters=hyperparameters,\n",
    "                                  train_volume_size=100) \n",
    "\n",
    "autogluon.fit({'training': s3_input_train})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Batch Inference\n",
    "\n",
    "At last, we're ready to make predictions. We're going to do so through a batch process and evaluate it's performance against our labels. We'll use Batch Transform to create churn scores which score the likelihood that our customers will churn.\n",
    "\n",
    "First, we upload the data we plan to use as batch input to S3. SageMaker Batch Transform is designed to run asynchronously and ingest input data from S3. This differs from SageMaker's real-time inference endpoints, which receive input data from synchronous HTTP requests.\n",
    "\n",
    "For large scale deployments the data set will be retrieved from Snowflake using SQL and an External Stage to S3.\n",
    "\n",
    "Batch Transform is often the ideal option for advanced analytics use case for several reasons:\n",
    "\n",
    " - Batch Transform is better optimized for throughput in comparison with real-time inference endpoints. Thus, Batch Transform is ideal for processing large volumes of data for analytics.\n",
    " - Offline asynchronous processing is acceptable for most analytics use cases.\n",
    " - Batch Transform is more cost efficient when real-time inference isn't necessary. You only need to pay for resources used during batch processing. There is no need to pay for ongoing resources like a hosted endpoint for real-time inference.\n",
    " \n",
    " ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input = churn.iloc[:,:-1]\n",
    "batch_input.to_csv('batch.csv', header=False, index=False)\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'batch/in/batch.csv')).upload_file('batch.csv')\n",
    "\n",
    "s3uri_batch_input ='s3://{}/{}/batch/in'.format(bucket, prefix)\n",
    "print('Batch Transform input S3 uri: {}'.format(s3uri_batch_input))\n",
    "\n",
    "s3uri_batch_output= 's3://{}/{}/batch/out'.format(bucket, prefix)\n",
    "print('Batch Transform output S3 uri: {}'.format(s3uri_batch_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now we create a transformer() object to describe our batch job. The transform() call results in an API call to the SageMaker service to create and run the batch process.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.transformer import Transformer\n",
    "BATCH_INSTANCE_TYPE = 'ml.c5.xlarge'\n",
    "\n",
    "transformer = autogluon.transformer(instance_count=1,\n",
    "                                         strategy='SingleRecord',\n",
    "                                         assemble_with='Line',\n",
    "                                         instance_type= BATCH_INSTANCE_TYPE,\n",
    "                                         accept = 'text/csv',\n",
    "                                         output_path=s3uri_batch_output)\n",
    "    \n",
    "transformer.transform(s3uri_batch_input,\n",
    "                      split_type= 'Line',\n",
    "                      content_type= 'text/csv',   \n",
    "                      input_filter = \"$[1:]\",\n",
    "                      join_source = \"Input\",\n",
    "                      output_filter = \"$[0,-1]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Batch transform jobs run asynchronously, and are non-blocking by default. Run the command below to block until the batch job completes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "There are many ways to compare the performance of a machine learning model, but let's start by simply by comparing actual to predicted values.  In this case, we're simply predicting whether the customer churned (`1`) or not (`0`), which produces a simple confusion matrix.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_churn_scores = pd.read_csv(s3uri_batch_output+'/batch.csv.out', usecols=[0,1], names=['id','scores'])\n",
    "batched_churn_scores['scores'] = (batched_churn_scores['scores'] == \"True.\").astype(int)\n",
    "#batched_churn_scores['Churn'] = (churn['Churn'] == \"True.\").astype(int)\n",
    "gt_df = pd.DataFrame((churn['Churn'] == \"True.\").astype(int)).reset_index(drop=True)\n",
    "\n",
    "results_df= pd.concat([gt_df,batched_churn_scores],axis=1)\n",
    "pd.crosstab(index=results_df['Churn'], columns=np.round(results_df['scores']), rownames=['actual'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Upload Churn Score to Snowflake\n",
    "\n",
    "To be able to allow multiple business users and dashboards simple access to the churn scores we will upload it to Snowflake by using a Snowflake internal stage. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "\n",
    "#Set the column names of the dataframe to match the table column names\n",
    "results_df.columns = ['CHURN_IN','CUST_ID','CHURN_SCORE']\n",
    "\n",
    "# Write the predictions to the table named \"ML_RESULTS\".\n",
    "success, nchunks, nrows, _ = write_pandas(ctx, results_df, 'ML_RESULTS')\n",
    "\n",
    "display(nrows)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
